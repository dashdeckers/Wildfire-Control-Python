% Climate change
@book{houghton1991climate,
  title={Climate change},
  author={Houghton, J. and Jenkins, T. and Ephraums, G.},
  publisher={Cambridge University Press},
  address={Cambridge (UK)},
  edition={3},
  year={1991}
}

% Basic forest fire/carbon stuff
@article{kasischke1995fire,
  title={Fire, global warming, and the carbon balance of boreal forests},
  author={Kasischke, E. and Christensen Jr, N. and Stocks, B.},
  journal={Ecological applications},
  volume={5},
  pages={437-451},
  year={1995},
}

% Original DQN Paper (well, second edition)
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei and Veness, Joel and G. Bellemare, Marc and Graves, Alex and Riedmiller, Martin and K. Fidjeland, Andreas and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}

% Policy correlation feedback loops
@inproceedings{tsitsiklis1997analysis,
  title={Analysis of temporal-diffference learning with function approximation},
  author={Tsitsiklis, J. and Van Roy, B.},
  booktitle={Advances in neural information processing systems},
  pages={1075-1081},
  year={1997}
}

% SARSA
@techreport{rummery1994line,
  title={On-line {Q}-learning using connectionist systems},
  author={Rummery, G. and Niranjan, M.},
  year={1994},
  institution={University of Cambridge, Department of Engineering Cambridge, England}
}

% Q-Learning
@phdthesis{watkins1989learning,
  title={Learning from delayed rewards},
  author={Watkins, C.},
  year={1989},
  school={University of Cambridge}
}

% The big book
@book{sutton_barto_2018, 
  title={Reinforcement Learning: an Introduction}, 
  publisher={The MIT Press}, 
  edition={2},
  author={Sutton, R. and Barto, A.}, 
  year={2018}
}

% Double Q-Learning
@inproceedings{hasselt2010double,
  title={Double {Q}-learning},
  author={van Hasselt, H.},
  editor={Lafferty, J. and Williams, C. and Shawe-Taylor, J. and Zemel, R. and Culotta, A.},
  booktitle={Advances in Neural Information Processing Systems},
  volume={23},
  pages={2613-2621},
  publisher={Curran Associates, Inc.},
  year={2010}
}

% Original Experience Replay
@Article{Lin1992,
  author="Lin, Long-Ji",
  title="Self-improving reactive agents based on reinforcement learning, planning and teaching",
  journal="Machine Learning",
  year="1992",
  month="May",
  day="01",
  volume="8",
  number="3",
  pages="293--321",
  abstract="To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.",
  issn="1573-0565",
  doi="10.1007/BF00992699",
  url="https://doi.org/10.1007/BF00992699"
}

% Hindsight actual benefits of experience replay
@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, T. and Quan, J. and Antonoglou, I. and Silver, D.},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

% Vision grids (Marco)
@inproceedings{knegt2018opponent,
  title={Opponent Modelling in the Game of {T}ron using Reinforcement Learning},
  author={Knegt, S.J.L. and Drugan, M.M. and Wiering, M.A.},
  booktitle = "ICAART 2018 - Proceedings of the 10th International Conference on Agents and Artificial Intelligence",
  volume = "2",
  pages = {29-40},
  year={2018}
}

% Dueling Q-Networks
@article{wang2015dueling,
  author    = {Ziyu Wang and
               Nando de Freitas and
               Marc Lanctot},
  title     = {Dueling Network Architectures for Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1511.06581},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.06581},
  archivePrefix = {arXiv},
  eprint    = {1511.06581},
}

% Wiering 1
@inproceedings{wiering1998learning,
  title={Learning to control forest fires},
  author={Wiering, M.A. and Doringo, M.},
  editor={Haasis, H. and Ranze, K.},
  year={1998},
  booktitle={Proceedings of the 12th international Symposium on 'Computer Science for Environmental Protection'},
  pages={378-388}
}

% Wiering 2
@inproceedings{wiering2005evolving,
  title={Evolving neural networks for forest fire control},
  author={Wiering, M.A. and Mignogna, F. and Maassen, B.},
  editor={van Otterlo, M. and Poel, M. and Nijholt, A.},
  booktitle={Benelearn '05: Proceedings of the 14th Belgian-Dutch Conference on Machine Learning},
  pages={113-120},
  year={2005}
}

% Reward shaping 2
@article{zou2019reward,
  title={Reward shaping via meta-learning},
  author={Zou, Haosheng and Ren, Tongzheng and Yan, Dong and Su, Hang and Zhu, Jun},
  journal={arXiv preprint arXiv:1901.09330},
  year={2019}
}

% Noisy nets
@article{fortunato2017noisy,
  title={Noisy networks for exploration},
  author    = {Meire Fortunato and
               Mohammad Gheshlaghi Azar and
               Bilal Piot and
               Jacob Menick and
               Ian Osband and
               Alex Graves and
               Vlad Mnih and
               R{\'{e}}mi Munos and
               Demis Hassabis and
               Olivier Pietquin and
               Charles Blundell and
               Shane Legg},
  journal={arXiv preprint arXiv:1706.10295},
  year={2017}
}

% Rainbow
@article{hessel2018rainbow,
  author    = {Matteo Hessel and
               Joseph Modayil and
               Hado van Hasselt and
               Tom Schaul and
               Georg Ostrovski and
               Will Dabney and
               Daniel Horgan and
               Bilal Piot and
               Mohammad Gheshlaghi Azar and
               David Silver},
  title     = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1710.02298},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.02298},
  archivePrefix = {arXiv},
  eprint    = {1710.02298},
}

% Other research
@inproceedings{moura2007fighting,
  title={Fighting fire with agents: an agent coordination model for simulated firefighting},
  author={Moura, Daniel and Oliveira, Eug{\'e}nio},
  booktitle={Proceedings of the 2007 spring simulation multiconference-Volume 2},
  pages={71-78},
  year={2007},
  organization={Society for Computer Simulation International}
}

% Reward shaping
@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={International Conference on Machine Learning},
  volume={99},
  pages={278-287},
  year={1999}
}

% Hindsight experience replay
@inproceedings{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, M. and Wolski, F. and Ray, A. and Schneider, J. and Fong, R. and Welinder, P. and McGrew, B. and Tobin, J. and Abbeel, P. and Zaremba, W.},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5048--5058},
  year={2017}
}

% DQV
@article{sabatelli2018deep,
  title={Deep quality-value (DQV) learning},
  author={Sabatelli, M. and Louppe, G. and Geurts, P. and Wiering, M.A.},
  journal={arXiv preprint arXiv:1810.00368},
  year={2018}
}