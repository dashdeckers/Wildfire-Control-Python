% Climate change
@book{houghton1991climate,
  title={Climate change},
  author={Houghton, John Theodore and Jenkins, Geoffrey J and Ephraums, Jim J},
  year={1991}
}
% Basic forest fire/carbon stuff
@article{kasischke1995fire,
  title={Fire, global warming, and the carbon balance of boreal forests},
  author={Kasischke, Eric S and Christensen Jr, NL and Stocks, Brian J},
  journal={Ecological applications},
  volume={5},
  number={2},
  pages={437--451},
  year={1995},
  publisher={Wiley Online Library}
}

% Original DQN Paper (well, second edition)
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}

% The big book
@book{sutton_barto_2018, 
  title={Reinforcement learning: an introduction}, 
  publisher={The MIT Press.}, 
  author={Sutton, Richard S.. and Barto, Andrew G..}, 
  year={2018}
 }

% Double Q-Learning (Target Networks)
@inproceedings{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado V},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2613--2621},
  year={2010}
}

% Original Experience Replay
@Article{Lin1992,
  author="Lin, Long-Ji",
  title="Self-improving reactive agents based on reinforcement learning, planning and teaching",
  journal="Machine Learning",
  year="1992",
  month="May",
  day="01",
  volume="8",
  number="3",
  pages="293--321",
  abstract="To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.",
  issn="1573-0565",
  doi="10.1007/BF00992699",
  url="https://doi.org/10.1007/BF00992699"
}

% Hindsight actual benefits of experience replay
@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

% Vision grids (Marco)
@inproceedings{knegt2018opponent,
  title={Opponent Modelling in the Game of Tron using Reinforcement Learning.},
  author={Knegt, Stefan JL and Drugan, Madalina M and Wiering, Marco A},
  booktitle={ICAART (2)},
  pages={29--40},
  year={2018}
}

% Dueling Q-Networks
@article{wang2015dueling,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Van Hasselt, Hado and Lanctot, Marc and De Freitas, Nando},
  journal={arXiv preprint arXiv:1511.06581},
  year={2015}
}

% Wiering 1
@article{wiering1998learning,
  title={Learning to control forest fires},
  author={Wiering, MA and Dorigo, Marco},
  year={1998},
  publisher={Metropolis Verlag}
}

%Wiering 2
@inproceedings{wiering2005evolving,
  title={Evolving neural networks for forest fire control},
  author={Wiering, MA and Mignogna, Fillipo and Maassen, Bernard},
  booktitle={Proceedings of the 14th Belgian-Dutch Conference on Machine Learning},
  pages={113--120},
  year={2005}
}
