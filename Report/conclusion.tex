%!TEX root = report.tex

\section{Conclusion}\label{sec:conclusions}
The simulation environment was very simplified. We have the functionality to determine the wind speed and direction but did not have the time to explore those options. There is also support for more cell types such as grass versus trees or rivers, each with different properties for a more complex and realistic environment. We believe this should be investigated in subsequent research, because the system should ultimately prove itself reliable in more complex environments (that is real life).

The reward function is, in its current form, very hard for an agent to learn with. It provides very sparse and very delayed rewards, this can be improved to provide a more smooth gradient and allowing for faster and more stable learning, and less reliance on demonstration data. 

Since all algorithms are based on connectionist reinforcement learning, they are likely to benefit from other improvements proven to be useful. Examples we think would perform well are prioritized experience replay \citep{schaul2015prioritized} and Deep Q-Networks \citep{mnih2015human}. Using a deep network with convolutional layers might extract some meaningful information that it cannot otherwise, like shape of the circle and position of the agent relative to it for example.

The results may have shown more interesting patterns for longer runs (more than 10,000 episodes). We might have seen the same patterns in performance for the 14-by-14 maps as for the 10-by-10 ones, just on a larger scale. It also likely that some patterns are not visible even on the smaller map size with 10000 episodes, such as the drop in performance for Q-Learning without Dueling networks.