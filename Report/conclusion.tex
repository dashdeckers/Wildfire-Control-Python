%!TEX root = report.tex

\section{Conclusion}\label{sec:conclusions}
The simulation environment was very simplified. We have the functionality to determine the wind speed and direction but did not have the time to explore those options. There is also support for more cell types such as grass versus trees or rivers, each with different properties for a more complex and realistic environment. We believe this should be investigated in subsequent research, because the system should ultimately prove itself reliable in more complex environments (that is real life).

The reward function is, in its current form, very hard for an agent to learn with. It provides very sparse and delayed rewards, this can be improved to provide a more smooth gradient and allowing for faster and more stable learning, and less reliance on demonstration data. There are also methods of introducing additional rewards while keeping the optimal policy identical (and in the case of multi-agent control, also keeping the Nash equilibria equal) \citep{ng1999policy}. A more general framework that can learn efficient reward shaping without the need for expert knowledge might has also been proposed \citep{zou2019reward}. Hindsight experience replay \citep{andrychowicz2017hindsight} might be an interesting way to deal with the sparse rewards, it allows the agent to learn from unsuccessful episodes (which there are a lot of) by imagining the negative outcome, in hindsight, to have been the goal all along.

Since all algorithms are based on connectionist reinforcement learning, they are likely to benefit from other improvements proven to be useful. Some we think would perform well include Deep $Q$-Networks \citep{mnih2015human}; Using a deep network with convolutional layers might extract some meaningful spatial information that it cannot otherwise, like shape of the circle and position of the agent relative to it for example. Prioritized experience replay \citep{schaul2015prioritized} might increase learning speed and performance due to a more efficient sampling of memories, and Double $Q$-Learning \citep{hasselt2010double} might reduce the overestimation bias introduced by the max operator in $Q$-Learning by using the main network to select actions and the target network to estimate the $Q$-values. Noisy nets \citep{fortunato2017noisy} have been shown to increase performance by replacing the e-greedy policy with parameterized noise in the network for a better exploration heuristic. For effective combinations of these improvements, see \cite{hessel2018rainbow}.

The results may have shown more interesting patterns for longer runs (more than 10,000 episodes). We might have seen the same patterns in performance for the 14-by-14 maps as for the 10-by-10 ones, just on a larger scale. It also likely that some patterns are not visible even on the smaller map size with 10000 episodes, such as the drop in performance for Q-Learning without Dueling networks.