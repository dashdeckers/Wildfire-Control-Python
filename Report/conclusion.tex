%!TEX root = report.tex

\section{Discussion}\label{sec:discussion}
The results show that all algorithms perform at least reasonably well (with the exception of some larger map sizes) and respond very differently and sometimes uniquely to the demonstration data given at the start of the training process. 

Starting with the 10-by-10 sized simulation (see Figures \ref{fig:10sized-nomem}, \ref{fig:10sized-100mem} and \ref{fig:10sized-1000mem}), Q-Networks performed best with 100 episodes of memories while the performance of Dueling Q-Networks was less dependent on the memories. The latter also showed to have a large spike in performance near the 1000th episode. This can be due to the memory buffer being filled completely by demonstration data. Because of this, the algorithm has very few chances to make mistakes and learn from those transitions during its exploration phase. It focuses too much on the demonstrated behaviour and is hindered in trying out actions to explore consequences. This explanation can also hold for why the Q-Network performs worse with 1000 episodes of memories.

SARSA showed to be one of the most consistent and trustworthy algorithms in terms of its response to the memories. In essence, more memories meant a higher sustained performance level. However, the speed at which it learned was decreased. This may also be due to the same reason mentioned; large amounts of memories hinder the algorithms exploration abilities. More memories also showed to have another positive effect on SARSA, namely its standard error. Higher performance levels came with lower standard errors. The general stability of SARSA compared to Q-Learning can be explained, as mentioned in Section \ref{sec:ql_sarsa}, by the fact that, because it is an on-policy algorithm, we effectively remove one of the three elements of the deadly triad. 

We found that combining Dueling Q-Networks and SARSA into Dueling SARSA proved to be and success and resulted in the overall best performer, especially in high memory scenarios. It inherits the best of both worlds; the learning speed from the first, and the performance and consistency from the latter. The algorithm inherited the performance peak from the Dueling Q-Networks, albeit not as high and slightly earlier. We estimate the maximum possible reward for an algorithm to achieve to be $\pm$1850. In Figure \ref{fig:10sized-1000mem} the algorithms performance gets very close to that maximum score and stays there throughout the rest of the training episodes. Moreover, its standard error there is similar to the baseline algorithm, which is impressive since the latter is a hard-coded solution.


When switching to a simulation consisting of a square grid of size 14 instead of 10 (see Figures \ref{fig:14sized-nomem} through \ref{fig:14sized-1000mem}), we see the algorithms struggle a lot more in general. This setting nearly doubles the amount of inputs for relatively small neural networks, which seem to struggle to properly learn and process all information. The number of episodes that the algorithm was allowed to learn for was also kept constant which could explain the low overall performances.

When the algorithms are not given any memories the Dueling Q-Networks is still able to perform better than the rest, but it achieves similar scores to the worst performers in the 10-by-10 simulation. Q-Networks, SARSA and Dueling SARSA seem to barely improve at all. When given 100 or 1000 episodes of memories the Dueling Q-Network improves in both cases, but does not achieve the same levels of performance. The Q-Network shows the same behaviour as before as it performs best with the medium amount of memories given and drops that advantage when given a high amount. This algorithm is also not able to solve the problem. Both Q-Network and Dueling Q-Networks fail to solve the problem and beat the baseline in all cases.

SARSA is still very receptive to memories with its best performance being the scenario where $\pm 48000$ memories were given. Here it is the only algorithm so far that is able to match the baseline algorithm. The only algorithm that is able to consistently beat it is Dueling SARSA in the high memory scenario. It shows performance very similar to the 10-by-10 simulation. The only difference is that it tops out later and suffers from a slightly higher standard error.

One last interesting thing to mention is that also in the 14-by-14 simulation Both Dueling Q-Networks and Dueling SARSA show a peak in performance at the start of the learning process. While the peak is now not as high, both algorithms have a peak of roughly the same size. The peaks are more spread out and Dueling SARSA's peak still happens earlier. In the 10-by-10 simulation both algorithms managed to quickly regain performance and continue learning after this peak, while in the 14-by-14 simulation only Dueling SARSA is able to continue learning properly.

\section{Conclusion}\label{sec:conclusions}
%What went well, what can be improved, other remarks etc
The simulation environment was very simplified. We have the functionality to determine the wind speed and direction but did not have the time to explore those options. There is also support for more cell types such as grass versus trees or rivers, each with different properties for a more complex and realistic environment. We believe this should be investigated in subsequent research, because the system should ultimately prove itself reliable in more complex environments (that is real life).

The reward function is, in its current form, very hard for an agent to learn with. It provides very sparse and very delayed rewards, this can be improved to provide a more smooth gradient and allowing for faster and more stable learning, and less reliance on demonstration data. 

Since all algorithms are based on connectionist reinforcement learning, they are likely to benefit from other improvements proven to be useful. Examples we think would perform well are prioritized experience replay \citep{schaul2015prioritized} and Deep Q-Networks \citep{mnih2015human}. Using a deep network with convolutional layers might extract some meaningful information that it cannot otherwise, like shape of the circle and position of the agent relative to it for example.

The results may have shown more interesting patterns for longer runs (more than 10,000 episodes). We might have seen the same patterns in performance for the 14-by-14 maps as for the 10-by-10 ones, just on a larger scale. It also likely that some patterns are not visible even on the smaller map size with 10000 episodes, such as the drop in performance for Q-Learning without Dueling networks.
