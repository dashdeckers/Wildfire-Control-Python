\section{Introduction}\label{sec:introduction}

\subsection{Forest Fires}
The ever increasing temperature around the globe due to global warming brings many consequences. One of which is the increased risk of forest fires. Warmer climates are not only plagued by forest fires more frequently, but also more intense forest fires.  In most cases, beginning wildfires are extinguished before they get out of hand. Sadly, some wildfires escalate into nearly uncontrollable infernos.

Fighting these forest fires is a challenging task. To kill a fire one or more of the three required elements has to be eliminated: fuel, heat or oxygen. The ordinary tactic is to remove the heat and oxygen by spraying water or foam from hoses, but large forest fires require more effort to be contained. Possible options include aircraft dropping water bombs, burning down specific areas in a controlled fashion, or using a bulldozer to cut fire lines. The use of these techniques need to be carefully planned by the fire-fighters when constructing a plan. To create the perfect plan is a near impossible job and it is not uncommon for it to fail and cause the loss of more forest.

Not only can forest fires result in the tragic loss of lives and houses, the ecological effect has to be taken into account as well. Trees and plants are a key factor in the carbon cycle \citep{kasischke1995fire}. Using photosynthesis massive amounts of CO$_{2}$ are filtered from the atmosphere and stored. When fires destroy large forests, all this stored CO$_{2}$ is released back into the atmosphere, which is inconsistent with the carbon cycle. Since this CO$_{2}$ is considered a greenhouse gas \citep{houghton1991climate} which boosts the already increasing global warming, this will increases the likelihood and risk of forest fires. The just described will result in a vicious cycle that is heading towards a warm, fiery planet that may not be able to support life in the future, in the same way it does today. 

Previous research focussed on how to construct a simulator for forest fires and how reinforcement learning algorithms could optimise policies by interaction with such a simulation \citep{wiering1998learning}. Other research explored how enforced sub-populations (ESP) could be used to evolve neural network controllers capable of solving the forest fire problem \citep{wiering2005evolving}.

In this paper we explore how forest fires can be simulated and connectionist reinforcement learning can be used on these simulations to let an agent learn to contain starting forest fires with the use of a bulldozer. We make use of existing algorithms: Q-Networks \citep{mnih2015human}, SARSA and Dueling Q-Networks \citep{wang2015dueling}. We show that using a simple baseline algorithm to generate demonstration data to be used in experience replay can greatly increase the algorithm's performance. We show that these algorithms based on Q-learning are able to complete this task successfully in small simulations. We also show that the latter two can be combined into Dueling SARSA which performs outperforms all, especially in simulations of a larger size where others fail.

%(more basic reasons why forest fires are bad)
%(more references to marcos papers)
%(more general information, definitions, analysis of what makes a forest fire and how to prevent it + references)
%(explain why cutting forest lines is the best choice + references)
%(explain why DQN is a good idea)

Research question: Do on-policy or off-policy methods, with function approximation, perform better at containing the spread of a fire by cutting fire lines in a simulated environment?

\subsection{Reinforcement Learning}\label{sec:reinforcementlearning}
Reinforcement learning is a machine learning paradigm typically consisting of two elements. The first is the agent, which represents the reinforcement learning algorithm or in other words, the machine. The second is the environment, which represents what the algorithm is trying to solve or at least perform quite well at. This is typically a game or in this case, a simulated forest fire that should be contained.

% insert (make your own) an image of agent <--> environment interaction including reward function
At each discrete time step (t=1,2,3...), the environment provides the agent with an observation (St) and a reward (Rt), which is obtained by passing the state to the reward function. Then, the agent interacts with the environment by choosing an action A from a set of possible actions, and observes the result of that action in (St+1) and (Rt+1). This interaction can be modelled by a Markov Decision Process, or MDP, as long as the Markov property holds: The probability of state (St+1) can be determined by only knowing the previous state (S). This property indeed holds, as the simulation only requires the current state to produce the next state.

The goal of the agent is to learn what actions to take, given access to the current state (St), to maximize the total cumulative reward from the initial state (S) to the terminal state (ST).

It can achieve this by learning the Q-values for each state, or in other words the quality values of each state corresponding to the expected cumulative reward that would be achieved by following the optimal policy from that state forward \citep{sutton_barto_2018}. The approximation of those Q-values is done via TD methods and the Bellman equations \citep{sutton_barto_2018}, and they would classically be stored in a Q-table but in deep reinforcement learning a deep neural network is used to approximate the function that takes as an input the state and outputs the estimated Q-values for each action. In this paper we use the DQN algorithm, which has been developed to do exactly that \citep{mnih2015human}, however we use a comparatively shallow network with a single hidden layer.
